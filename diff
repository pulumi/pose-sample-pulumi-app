diff --git a/.github/workflows/deploy.yml b/.github/workflows/deploy.yml
new file mode 100644
index 0000000..47a2ac1
--- /dev/null
+++ b/.github/workflows/deploy.yml
@@ -0,0 +1,167 @@
+name: Deploy PyTorch Training Cluster
+
+on:
+  push:
+    branches: [main]
+  pull_request:
+    branches: [main]
+  workflow_dispatch:
+    inputs:
+      environment:
+        description: 'Environment to deploy to'
+        required: true
+        default: 'dev'
+        type: choice
+        options:
+        - dev
+        - staging
+        - prod
+
+env:
+  PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}
+
+jobs:
+  preview:
+    name: Preview Infrastructure Changes
+    runs-on: ubuntu-latest
+    if: github.event_name == 'pull_request'
+    steps:
+    - name: Checkout code
+      uses: actions/checkout@v4
+
+    - name: Setup Node.js
+      uses: actions/setup-node@v4
+      with:
+        node-version: '18'
+        cache: 'npm'
+
+    - name: Install dependencies
+      run: npm ci
+
+    - name: Authenticate to Google Cloud
+      uses: google-github-actions/auth@v2
+      with:
+        credentials_json: ${{ secrets.GCP_DEV_SANDBOX_SA_KEY }}
+
+    - name: Setup Google Cloud SDK
+      uses: google-github-actions/setup-gcloud@v2
+
+    - name: Install Pulumi CLI
+      uses: pulumi/actions@v4
+
+    - name: Pulumi Preview
+      uses: pulumi/actions@v4
+      with:
+        command: preview
+        stack-name: dev
+        comment-on-pr: true
+        github-token: ${{ secrets.GITHUB_TOKEN }}
+      env:
+        GOOGLE_CREDENTIALS: ${{ secrets.GCP_DEV_SANDBOX_SA_KEY }}
+
+  deploy:
+    name: Deploy Infrastructure
+    runs-on: ubuntu-latest
+    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
+    environment: ${{ github.event.inputs.environment || 'dev' }}
+    steps:
+    - name: Checkout code
+      uses: actions/checkout@v4
+
+    - name: Setup Node.js
+      uses: actions/setup-node@v4
+      with:
+        node-version: '18'
+        cache: 'npm'
+
+    - name: Install dependencies
+      run: npm ci
+
+    - name: Authenticate to Google Cloud
+      uses: google-github-actions/auth@v2
+      with:
+        credentials_json: ${{ secrets.GCP_DEV_SANDBOX_SA_KEY }}
+
+    - name: Setup Google Cloud SDK
+      uses: google-github-actions/setup-gcloud@v2
+
+    - name: Install Pulumi CLI
+      uses: pulumi/actions@v4
+
+    - name: Configure Pulumi Stack
+      run: |
+        pulumi stack select ${{ github.event.inputs.environment || 'dev' }} || pulumi stack init ${{ github.event.inputs.environment || 'dev' }}
+        pulumi config set gcp:project ${{ vars.GCP_PROJECT_ID }}
+        pulumi config set gcp:region ${{ vars.GCP_REGION || 'us-central1' }}
+        pulumi config set gcp:zone ${{ vars.GCP_ZONE || 'us-central1-a' }}
+
+    - name: Pulumi Up
+      uses: pulumi/actions@v4
+      with:
+        command: up
+        stack-name: ${{ github.event.inputs.environment || 'dev' }}
+      env:
+        GOOGLE_CREDENTIALS: ${{ secrets.GCP_DEV_SANDBOX_SA_KEY }}
+
+    - name: Get cluster credentials
+      run: |
+        CLUSTER_NAME=$(pulumi stack output clusterName)
+        CLUSTER_LOCATION=$(pulumi stack output clusterLocation)
+        gcloud container clusters get-credentials $CLUSTER_NAME --location=$CLUSTER_LOCATION
+
+    - name: Deploy PyTorch training resources
+      run: |
+        kubectl apply -f pytorch-training/sample-training-job.yaml
+        kubectl apply -f pytorch-training/monitoring.yaml
+
+    - name: Verify deployment
+      run: |
+        kubectl get nodes -l accelerator=nvidia-tesla-t4
+        kubectl get pods -n pytorch-training
+        kubectl get pvc -n pytorch-training
+
+  test-training:
+    name: Test PyTorch Training
+    runs-on: ubuntu-latest
+    needs: deploy
+    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
+    steps:
+    - name: Checkout code
+      uses: actions/checkout@v4
+
+    - name: Authenticate to Google Cloud
+      uses: google-github-actions/auth@v2
+      with:
+        credentials_json: ${{ secrets.GCP_DEV_SANDBOX_SA_KEY }}
+
+    - name: Setup Google Cloud SDK
+      uses: google-github-actions/setup-gcloud@v2
+
+    - name: Install Pulumi CLI
+      uses: pulumi/actions@v4
+
+    - name: Get cluster credentials
+      run: |
+        pulumi stack select ${{ github.event.inputs.environment || 'dev' }}
+        CLUSTER_NAME=$(pulumi stack output clusterName)
+        CLUSTER_LOCATION=$(pulumi stack output clusterLocation)
+        gcloud container clusters get-credentials $CLUSTER_NAME --location=$CLUSTER_LOCATION
+
+    - name: Run sample training job
+      run: |
+        # Delete any existing job
+        kubectl delete job pytorch-training-sample -n pytorch-training --ignore-not-found=true
+        
+        # Apply the sample training job
+        kubectl apply -f pytorch-training/sample-training-job.yaml
+        
+        # Wait for job to complete (timeout after 10 minutes)
+        kubectl wait --for=condition=complete job/pytorch-training-sample -n pytorch-training --timeout=600s
+        
+        # Show job logs
+        kubectl logs job/pytorch-training-sample -n pytorch-training
+
+    - name: Cleanup test job
+      if: always()
+      run: |
+        kubectl delete job pytorch-training-sample -n pytorch-training --ignore-not-found=true
\ No newline at end of file
diff --git a/.gitignore b/.gitignore
index 50ea185..e84442a 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,2 +1,34 @@
-/node_modules/
-/bin/
+node_modules/
+*.log
+.DS_Store
+.env
+.env.local
+.env.*.local
+npm-debug.log*
+yarn-debug.log*
+yarn-error.log*
+
+# Pulumi
+.pulumi/
+Pulumi.*.yaml.bak
+
+# IDE
+.vscode/
+.idea/
+*.swp
+*.swo
+
+# OS
+Thumbs.db
+
+# Temporary files
+*.tmp
+*.temp
+
+# Build outputs
+dist/
+build/
+
+# Kubernetes
+kubeconfig
+*.kubeconfig
\ No newline at end of file
diff --git a/Pulumi.dev.yaml b/Pulumi.dev.yaml
new file mode 100644
index 0000000..0b83bf3
--- /dev/null
+++ b/Pulumi.dev.yaml
@@ -0,0 +1,6 @@
+config:
+  gcp:project: your-gcp-project-id
+  gcp:region: us-central1
+  gcp:zone: us-central1-a
+  pytorch-training-cluster:region: us-central1
+  pytorch-training-cluster:zone: us-central1-a
\ No newline at end of file
diff --git a/Pulumi.yaml b/Pulumi.yaml
index 2675b2f..df9f844 100644
--- a/Pulumi.yaml
+++ b/Pulumi.yaml
@@ -1,7 +1,7 @@
-name: pose-sample-app
-description: A simple AWS serverless JavaScript Pulumi program
+name: pytorch-training-cluster
+description: A GKE-based PyTorch training cluster for development and production workloads
 runtime:
   name: nodejs
   options:
     typescript: true
-    packagemanager: npm
+    packagemanager: npm
\ No newline at end of file
diff --git a/README.md b/README.md
new file mode 100644
index 0000000..862425b
--- /dev/null
+++ b/README.md
@@ -0,0 +1,222 @@
+# PyTorch Training Cluster on GKE
+
+A canonical development PyTorch cluster built on Google Kubernetes Engine (GKE) with GPU support, designed for cost-effective development and ready for production-scale training.
+
+## Architecture Overview
+
+### Infrastructure Components
+
+- **GKE Cluster**: Regional cluster with private nodes and workload identity
+- **System Node Pool**: Cost-effective nodes (e2-medium) for Kubernetes system components
+- **GPU Node Pool**: NVIDIA T4 GPU nodes (n1-standard-4) for training workloads
+- **Networking**: VPC-native networking with private cluster configuration
+- **Storage**: Persistent volumes for datasets, models, and checkpoints
+
+### Cost Optimization Features
+
+- **Development-focused**: Starts with 0 GPU nodes, scales up on demand
+- **NVIDIA T4 GPUs**: Cost-effective GPU choice for development workloads
+- **Autoscaling**: Automatic scaling based on workload demands
+- **Resource Quotas**: Prevents runaway costs with defined limits
+
+### Production-Ready Design
+
+- **Scalable Architecture**: Easy transition to more powerful GPU types
+- **Monitoring Integration**: Built-in monitoring and logging
+- **CI/CD Pipeline**: Automated deployments via GitHub Actions
+- **Security**: Private cluster with workload identity and proper RBAC
+
+## Quick Start
+
+### Prerequisites
+
+1. Google Cloud Project with billing enabled
+2. Required APIs enabled:
+   - Kubernetes Engine API
+   - Compute Engine API
+   - Container Registry API
+3. Service account with appropriate permissions
+4. Pulumi account and access token
+
+### Deployment
+
+1. **Configure the project**:
+   ```bash
+   # Update Pulumi.dev.yaml with your GCP project ID
+   pulumi config set gcp:project your-gcp-project-id
+   pulumi config set gcp:region us-central1
+   ```
+
+2. **Deploy the infrastructure**:
+   ```bash
+   npm install
+   pulumi up
+   ```
+
+3. **Get cluster credentials**:
+   ```bash
+   CLUSTER_NAME=$(pulumi stack output clusterName)
+   CLUSTER_LOCATION=$(pulumi stack output clusterLocation)
+   gcloud container clusters get-credentials $CLUSTER_NAME --location=$CLUSTER_LOCATION
+   ```
+
+4. **Deploy training resources**:
+   ```bash
+   kubectl apply -f pytorch-training/sample-training-job.yaml
+   kubectl apply -f pytorch-training/monitoring.yaml
+   ```
+
+### Running Your First Training Job
+
+```bash
+# Check if GPU nodes are available
+kubectl get nodes -l accelerator=nvidia-tesla-t4
+
+# Run the sample training job
+kubectl apply -f pytorch-training/sample-training-job.yaml
+
+# Monitor the job
+kubectl get jobs -n pytorch-training
+kubectl logs job/pytorch-training-sample -n pytorch-training -f
+```
+
+## Development Workflow
+
+### Local Development
+
+1. **Test infrastructure changes**:
+   ```bash
+   pulumi preview
+   ```
+
+2. **Deploy changes**:
+   ```bash
+   pulumi up
+   ```
+
+3. **Run training experiments**:
+   ```bash
+   kubectl apply -f your-training-job.yaml
+   ```
+
+### CI/CD Pipeline
+
+The GitHub Actions workflow automatically:
+- Previews infrastructure changes on PRs
+- Deploys to development environment on main branch pushes
+- Runs integration tests with sample training jobs
+- Supports manual deployments to staging/production
+
+## Scaling for Production
+
+### Automatic Scaling
+
+The cluster includes a production scaling script:
+
+```bash
+./scripts/scale-cluster.sh \
+  --cluster-name your-cluster-name \
+  --location us-central1 \
+  --max-nodes 20 \
+  --machine-type n1-standard-8 \
+  --gpu-type nvidia-tesla-v100
+```
+
+### Production Configuration
+
+For production workloads, consider:
+
+1. **Upgrade GPU types**: T4 → V100 → A100
+2. **Increase node sizes**: n1-standard-4 → n1-standard-8+
+3. **Add more storage**: Increase PVC sizes for larger datasets
+4. **Enable monitoring**: Set up Prometheus and Grafana
+5. **Configure alerts**: Set up cost and performance alerts
+
+### Resource Templates
+
+Use the production training template:
+
+```bash
+# Set environment variables
+export MODEL_NAME="resnet50"
+export EXPERIMENT_ID="exp-001"
+export EPOCHS="100"
+export BATCH_SIZE="64"
+
+# Apply with substitutions
+envsubst < pytorch-training/production-training-template.yaml | kubectl apply -f -
+```
+
+## Monitoring and Observability
+
+### Cluster Monitoring
+
+- **Google Cloud Monitoring**: Automatic cluster and node metrics
+- **Kubernetes Dashboard**: Web UI for cluster management
+- **Resource Usage**: Built-in resource quota monitoring
+
+### Training Job Monitoring
+
+- **Job Logs**: Centralized logging via Cloud Logging
+- **Custom Metrics**: Prometheus integration for training metrics
+- **GPU Utilization**: NVIDIA GPU metrics collection
+
+### Cost Monitoring
+
+- **Resource Quotas**: Prevent cost overruns
+- **Billing Alerts**: Set up budget alerts in Google Cloud
+- **Usage Reports**: Regular cost analysis and optimization
+
+## Security Features
+
+- **Private Cluster**: Nodes have no external IP addresses
+- **Workload Identity**: Secure pod-to-GCP service authentication
+- **Network Policies**: Kubernetes network segmentation
+- **RBAC**: Role-based access control for training namespaces
+- **Pod Security**: Security contexts and admission controllers
+
+## Troubleshooting
+
+### Common Issues
+
+1. **GPU nodes not starting**:
+   ```bash
+   # Check node pool status
+   kubectl describe nodes
+   # Check GPU availability in your zone
+   gcloud compute accelerator-types list --zones=us-central1-a
+   ```
+
+2. **Training jobs stuck in pending**:
+   ```bash
+   # Check resource requests vs. available resources
+   kubectl describe pod -n pytorch-training
+   # Check node selectors and tolerations
+   ```
+
+3. **Out of quota errors**:
+   ```bash
+   # Check current quotas
+   gcloud compute project-info describe --project=your-project
+   # Request quota increases if needed
+   ```
+
+### Getting Help
+
+- Check the [Pulumi GCP documentation](https://www.pulumi.com/docs/clouds/gcp/)
+- Review [GKE best practices](https://cloud.google.com/kubernetes-engine/docs/best-practices)
+- Consult [PyTorch on Kubernetes guides](https://pytorch.org/tutorials/intermediate/kubernetes_tutorial.html)
+
+## Contributing
+
+1. Fork the repository
+2. Create a feature branch
+3. Make your changes
+4. Test with `pulumi preview`
+5. Submit a pull request
+
+The CI/CD pipeline will automatically test your changes and provide feedback.
+
+## License
+
+This project is licensed under the MIT License - see the LICENSE file for details.
\ No newline at end of file
diff --git a/index.ts b/index.ts
index 0550ab8..9a3dd82 100644
--- a/index.ts
+++ b/index.ts
@@ -1,61 +1,292 @@
-import * as apigateway from "@pulumi/aws-apigateway";
-import * as aws from "@pulumi/aws";
-import { Runtime } from "@pulumi/aws/lambda";
-
-const eventHandler = new aws.lambda.CallbackFunction("handler", {
-    runtime: Runtime.NodeJS18dX,
-    callback: async () => {
-        // Collection of dad jokes
-        const dadJokes = [
-            "Why don't scientists trust atoms? Because they make up everything!",
-            "Why did the scarecrow win an award? He was outstanding in his field!",
-            "Why don't eggs tell jokes? They'd crack each other up!",
-            "I told my wife she was drawing her eyebrows too high. She looked surprised!",
-            "Why can't a bicycle stand up by itself? It's two tired!",
-            "What do you call a fake noodle? An impasta!",
-            "Want to hear a construction joke? Oh never mind, I'm still working on that one!",
-            "Why did the coffee file a police report? It got mugged!",
-            "How do you organize a space party? You planet!",
-            "Why did the math book look so sad? Because of all of its problems!",
-            "What do you call cheese that isn't yours? Nacho cheese!",
-            "Why couldn't the leopard play hide and seek? Because he was always spotted!",
-            "What do you call a bear with no teeth? A gummy bear!",
-            "Why did the cookie go to the doctor? Because it felt crumbly!",
-            "How does a penguin build its house? Igloos it together!",
-            "Why don't skeletons fight each other? They don't have the guts!",
-            "What did the grape do when he got stepped on? Nothing but let out a little wine!",
-            "I used to hate facial hair... but then it grew on me!",
-            "What do you call a dinosaur that crashes his car? Tyrannosaurus Wrecks!",
-            "Why can't your nose be 12 inches long? Because then it would be a foot!"
-        ];
-        
-        // Select a random dad joke
-        const randomJoke = dadJokes[Math.floor(Math.random() * dadJokes.length)];
-        
-        return {
-            statusCode: 200,
-            body: JSON.stringify({
-                joke: randomJoke,
-                timestamp: new Date().toISOString(),
-            }),
-        };
+import * as gcp from "@pulumi/gcp";
+import * as k8s from "@pulumi/kubernetes";
+import * as random from "@pulumi/random";
+
+// Configuration
+const config = new gcp.Config();
+const projectId = gcp.config.project || "your-gcp-project";
+const region = config.get("region") || "us-central1";
+const zone = config.get("zone") || "us-central1-a";
+
+// Random suffix for unique resource names
+const suffix = new random.RandomString("suffix", {
+    length: 8,
+    special: false,
+    upper: false,
+});
+
+// GKE Cluster for PyTorch Training
+const cluster = new gcp.container.Cluster("pytorch-cluster", {
+    name: suffix.result.apply(s => `pytorch-cluster-${s}`),
+    location: region,
+    
+    // Remove default node pool - we'll create custom ones
+    removeDefaultNodePool: true,
+    initialNodeCount: 1,
+    
+    // Network configuration for security
+    networkingMode: "VPC_NATIVE",
+    ipAllocationPolicy: {
+        clusterIpv4CidrBlock: "10.0.0.0/14",
+        servicesIpv4CidrBlock: "10.4.0.0/19",
+    },
+    
+    // Private cluster configuration
+    privateClusterConfig: {
+        enablePrivateNodes: true,
+        enablePrivateEndpoint: false, // Allow public access for development
+        masterIpv4CidrBlock: "10.5.0.0/28",
+    },
+    
+    // Enable workload identity for secure pod-to-GCP authentication
+    workloadIdentityConfig: {
+        workloadPool: `${projectId}.svc.id.goog`,
+    },
+    
+    // Cluster features
+    addonsConfig: {
+        horizontalPodAutoscaling: { disabled: false },
+        httpLoadBalancing: { disabled: false },
+        networkPolicyConfig: { disabled: false },
+    },
+    
+    // Enable logging and monitoring
+    loggingService: "logging.googleapis.com/kubernetes",
+    monitoringService: "monitoring.googleapis.com/kubernetes",
+    
+    // Resource labels
+    resourceLabels: {
+        environment: "development",
+        purpose: "pytorch-training",
+        team: "ml-engineering",
     },
 });
 
-const endpoint = new apigateway.RestAPI("api", {
-    routes: [
-        {
-            path: "/source",
-            method: "GET",
-            eventHandler,
+// System Node Pool (for Kubernetes system components)
+const systemNodePool = new gcp.container.NodePool("system-pool", {
+    name: "system-pool",
+    cluster: cluster.name,
+    location: region,
+    
+    nodeCount: 1,
+    
+    nodeConfig: {
+        machineType: "e2-medium", // Cost-effective for system workloads
+        diskSizeGb: 50,
+        diskType: "pd-standard",
+        
+        // Use Container-Optimized OS
+        imageType: "COS_CONTAINERD",
+        
+        // Security settings
+        serviceAccount: "default",
+        oauthScopes: [
+            "https://www.googleapis.com/auth/cloud-platform",
+        ],
+        
+        // Node labels and taints for system workloads
+        labels: {
+            "node-type": "system",
+            "workload": "system",
+        },
+        
+        taint: [{
+            key: "node-type",
+            value: "system",
+            effect: "NO_SCHEDULE",
+        }],
+        
+        metadata: {
+            "disable-legacy-endpoints": "true",
+        },
+    },
+    
+    management: {
+        autoRepair: true,
+        autoUpgrade: true,
+    },
+});
+
+// GPU Node Pool (for PyTorch training workloads)
+const gpuNodePool = new gcp.container.NodePool("gpu-pool", {
+    name: "gpu-pool",
+    cluster: cluster.name,
+    location: zone, // GPU nodes need to be in specific zones
+    
+    // Start with 0 nodes for cost savings, scale up when needed
+    initialNodeCount: 0,
+    
+    autoscaling: {
+        minNodeCount: 0,
+        maxNodeCount: 3, // Limit for development environment
+    },
+    
+    nodeConfig: {
+        machineType: "n1-standard-4", // Good balance for T4 GPU
+        diskSizeGb: 100,
+        diskType: "pd-ssd", // SSD for better I/O performance
+        
+        // NVIDIA T4 GPU (cost-effective for development)
+        guestAccelerator: [{
+            type: "nvidia-tesla-t4",
+            count: 1,
+        }],
+        
+        // Use Container-Optimized OS with GPU support
+        imageType: "COS_CONTAINERD",
+        
+        // Security settings
+        serviceAccount: "default",
+        oauthScopes: [
+            "https://www.googleapis.com/auth/cloud-platform",
+        ],
+        
+        // Node labels for GPU workloads
+        labels: {
+            "node-type": "gpu",
+            "workload": "training",
+            "accelerator": "nvidia-tesla-t4",
         },
-        {
-            path: "/",
-            localPath: "www",
+        
+        taint: [{
+            key: "nvidia.com/gpu",
+            value: "present",
+            effect: "NO_SCHEDULE",
+        }],
+        
+        metadata: {
+            "disable-legacy-endpoints": "true",
         },
-    ],
+    },
+    
+    management: {
+        autoRepair: true,
+        autoUpgrade: true,
+    },
+});
+
+// Kubernetes provider
+const k8sProvider = new k8s.Provider("gke-k8s", {
+    kubeconfig: cluster.name.apply(name => 
+        gcp.container.getClusterKubeconfig({
+            name: name,
+            location: region,
+        }).then(result => result.kubeconfig)
+    ),
 });
 
+// Namespace for PyTorch training workloads
+const trainingNamespace = new k8s.core.v1.Namespace("pytorch-training", {
+    metadata: {
+        name: "pytorch-training",
+        labels: {
+            purpose: "ml-training",
+            environment: "development",
+        },
+    },
+}, { provider: k8sProvider });
+
+// Resource quota for cost control
+const resourceQuota = new k8s.core.v1.ResourceQuota("training-quota", {
+    metadata: {
+        namespace: trainingNamespace.metadata.name,
+        name: "training-resource-quota",
+    },
+    spec: {
+        hard: {
+            "requests.cpu": "8",
+            "requests.memory": "32Gi",
+            "requests.nvidia.com/gpu": "2",
+            "limits.cpu": "16",
+            "limits.memory": "64Gi",
+            "limits.nvidia.com/gpu": "2",
+            "persistentvolumeclaims": "5",
+        },
+    },
+}, { provider: k8sProvider });
+
+// NVIDIA device plugin for GPU support
+const nvidiaDevicePlugin = new k8s.apps.v1.DaemonSet("nvidia-device-plugin", {
+    metadata: {
+        name: "nvidia-device-plugin-daemonset",
+        namespace: "kube-system",
+    },
+    spec: {
+        selector: {
+            matchLabels: {
+                name: "nvidia-device-plugin-ds",
+            },
+        },
+        updateStrategy: {
+            type: "RollingUpdate",
+        },
+        template: {
+            metadata: {
+                labels: {
+                    name: "nvidia-device-plugin-ds",
+                },
+            },
+            spec: {
+                tolerations: [{
+                    key: "nvidia.com/gpu",
+                    operator: "Exists",
+                    effect: "NoSchedule",
+                }],
+                nodeSelector: {
+                    "accelerator": "nvidia-tesla-t4",
+                },
+                priorityClassName: "system-node-critical",
+                containers: [{
+                    image: "nvcr.io/nvidia/k8s-device-plugin:v0.14.1",
+                    name: "nvidia-device-plugin-ctr",
+                    args: ["--fail-on-init-error=false"],
+                    securityContext: {
+                        allowPrivilegeEscalation: false,
+                        capabilities: {
+                            drop: ["ALL"],
+                        },
+                    },
+                    volumeMounts: [{
+                        name: "device-plugin",
+                        mountPath: "/var/lib/kubelet/device-plugins",
+                    }],
+                }],
+                volumes: [{
+                    name: "device-plugin",
+                    hostPath: {
+                        path: "/var/lib/kubelet/device-plugins",
+                    },
+                }],
+            },
+        },
+    },
+}, { provider: k8sProvider });
+
+// Persistent Volume for datasets and model storage
+const datasetPVC = new k8s.core.v1.PersistentVolumeClaim("pytorch-datasets", {
+    metadata: {
+        namespace: trainingNamespace.metadata.name,
+        name: "pytorch-datasets",
+    },
+    spec: {
+        accessModes: ["ReadWriteOnce"],
+        resources: {
+            requests: {
+                storage: "100Gi",
+            },
+        },
+        storageClassName: "standard-rwo",
+    },
+}, { provider: k8sProvider });
 
-// Export the public URL for the HTTP service
-export const url = endpoint.url;
+// Export important values
+export const clusterName = cluster.name;
+export const clusterEndpoint = cluster.endpoint;
+export const clusterLocation = cluster.location;
+export const kubeconfig = cluster.name.apply(name => 
+    gcp.container.getClusterKubeconfig({
+        name: name,
+        location: region,
+    }).then(result => result.kubeconfig)
+);
+export const trainingNamespaceName = trainingNamespace.metadata.name;
\ No newline at end of file
diff --git a/package.json b/package.json
index f9a3591..d58fc8f 100644
--- a/package.json
+++ b/package.json
@@ -1,11 +1,13 @@
 {
-    "name": "pose-sample-app",
+    "name": "pytorch-training-cluster",
+    "description": "GKE-based PyTorch training cluster infrastructure",
     "dependencies": {
-        "@pulumi/aws": "^7.0.0",
-        "@pulumi/aws-apigateway": "^3.0.0"
+        "@pulumi/gcp": "^8.0.0",
+        "@pulumi/kubernetes": "^4.0.0",
+        "@pulumi/random": "^4.0.0"
     },
     "devDependencies": {
         "@types/node": "^18.0.0",
         "typescript": "^4.9.0"
     }
-}
+}
\ No newline at end of file
diff --git a/production/cluster-config.yaml b/production/cluster-config.yaml
new file mode 100644
index 0000000..81e9c05
--- /dev/null
+++ b/production/cluster-config.yaml
@@ -0,0 +1,58 @@
+# Production cluster configuration template
+# Use this to scale up for production workloads
+
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: production-cluster-config
+  namespace: pytorch-training
+data:
+  # Cluster scaling parameters
+  min-gpu-nodes: "2"
+  max-gpu-nodes: "50"
+  gpu-machine-type: "n1-standard-8"  # or a100-megagpu-40g for high-end training
+  gpu-type: "nvidia-tesla-v100"      # or nvidia-tesla-a100 for cutting-edge performance
+  gpu-count-per-node: "2"
+  
+  # Storage configuration
+  dataset-storage: "1Ti"
+  model-storage: "500Gi"
+  checkpoint-storage: "2Ti"
+  
+  # Training job defaults
+  default-memory-request: "16Gi"
+  default-memory-limit: "32Gi"
+  default-cpu-request: "8"
+  default-cpu-limit: "16"
+  default-gpu-request: "1"
+  default-gpu-limit: "2"
+  
+  # Monitoring and logging
+  enable-prometheus: "true"
+  enable-grafana: "true"
+  log-retention-days: "30"
+  metrics-retention-days: "90"
+---
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: cost-optimization-config
+  namespace: pytorch-training
+data:
+  # Cost optimization settings
+  enable-preemptible-nodes: "true"
+  preemptible-percentage: "70"  # 70% preemptible, 30% regular
+  enable-cluster-autoscaler: "true"
+  scale-down-delay: "10m"
+  scale-down-unneeded-time: "10m"
+  
+  # Resource quotas for cost control
+  max-cpu-quota: "1000"
+  max-memory-quota: "4000Gi"
+  max-gpu-quota: "20"
+  max-storage-quota: "10Ti"
+  
+  # Scheduling preferences
+  enable-node-affinity: "true"
+  prefer-spot-instances: "true"
+  enable-pod-disruption-budgets: "true"
\ No newline at end of file
diff --git a/production/distributed-training.yaml b/production/distributed-training.yaml
new file mode 100644
index 0000000..8bb763c
--- /dev/null
+++ b/production/distributed-training.yaml
@@ -0,0 +1,97 @@
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: distributed-pytorch-training
+  namespace: pytorch-training
+  labels:
+    app: pytorch-training
+    job-type: distributed
+    framework: pytorch
+spec:
+  parallelism: 4  # Number of worker pods
+  completions: 4
+  template:
+    metadata:
+      labels:
+        app: pytorch-training
+        job-type: distributed
+    spec:
+      restartPolicy: Never
+      tolerations:
+      - key: nvidia.com/gpu
+        operator: Exists
+        effect: NoSchedule
+      nodeSelector:
+        accelerator: nvidia-tesla-v100
+      containers:
+      - name: pytorch-trainer
+        image: pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel
+        command: ["python", "-m", "torch.distributed.launch"]
+        args:
+        - --nproc_per_node=2
+        - --nnodes=4
+        - --node_rank=$(POD_INDEX)
+        - --master_addr=$(MASTER_ADDR)
+        - --master_port=23456
+        - /app/distributed_train.py
+        - --epochs=100
+        - --batch-size=32
+        - --data-path=/datasets
+        - --output-path=/models
+        env:
+        - name: POD_INDEX
+          valueFrom:
+            fieldRef:
+              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
+        - name: MASTER_ADDR
+          value: "distributed-pytorch-training-0"
+        - name: NCCL_DEBUG
+          value: "INFO"
+        - name: NCCL_SOCKET_IFNAME
+          value: "eth0"
+        - name: PYTHONUNBUFFERED
+          value: "1"
+        resources:
+          requests:
+            memory: "32Gi"
+            cpu: "16"
+            nvidia.com/gpu: 2
+          limits:
+            memory: "64Gi"
+            cpu: "32"
+            nvidia.com/gpu: 2
+        volumeMounts:
+        - name: datasets
+          mountPath: /datasets
+          readOnly: true
+        - name: models
+          mountPath: /models
+        - name: shared-memory
+          mountPath: /dev/shm
+      volumes:
+      - name: datasets
+        persistentVolumeClaim:
+          claimName: pytorch-datasets
+      - name: models
+        persistentVolumeClaim:
+          claimName: pytorch-models
+      - name: shared-memory
+        emptyDir:
+          medium: Memory
+          sizeLimit: 8Gi
+  backoffLimit: 2
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: distributed-pytorch-training
+  namespace: pytorch-training
+spec:
+  clusterIP: None  # Headless service for distributed training
+  selector:
+    app: pytorch-training
+    job-type: distributed
+  ports:
+  - name: master-port
+    port: 23456
+    targetPort: 23456
\ No newline at end of file
diff --git a/pytorch-training/monitoring.yaml b/pytorch-training/monitoring.yaml
new file mode 100644
index 0000000..9f7737d
--- /dev/null
+++ b/pytorch-training/monitoring.yaml
@@ -0,0 +1,100 @@
+apiVersion: v1
+kind: ServiceMonitor
+metadata:
+  name: pytorch-training-metrics
+  namespace: pytorch-training
+  labels:
+    app: pytorch-training
+spec:
+  selector:
+    matchLabels:
+      app: pytorch-training
+  endpoints:
+  - port: metrics
+    interval: 30s
+    path: /metrics
+---
+apiVersion: v1
+kind: Service
+metadata:
+  name: pytorch-training-metrics
+  namespace: pytorch-training
+  labels:
+    app: pytorch-training
+spec:
+  ports:
+  - name: metrics
+    port: 8080
+    targetPort: 8080
+  selector:
+    app: pytorch-training
+---
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: training-monitoring-config
+  namespace: pytorch-training
+data:
+  prometheus.yml: |
+    global:
+      scrape_interval: 15s
+    scrape_configs:
+    - job_name: 'pytorch-training'
+      kubernetes_sd_configs:
+      - role: pod
+        namespaces:
+          names:
+          - pytorch-training
+      relabel_configs:
+      - source_labels: [__meta_kubernetes_pod_label_app]
+        action: keep
+        regex: pytorch-training
+      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
+        action: keep
+        regex: true
+      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
+        action: replace
+        target_label: __metrics_path__
+        regex: (.+)
+      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
+        action: replace
+        regex: ([^:]+)(?::\d+)?;(\d+)
+        replacement: $1:$2
+        target_label: __address__
+---
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: training-log-collector
+  namespace: pytorch-training
+spec:
+  replicas: 1
+  selector:
+    matchLabels:
+      app: training-log-collector
+  template:
+    metadata:
+      labels:
+        app: training-log-collector
+    spec:
+      containers:
+      - name: fluentd
+        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
+        env:
+        - name: FLUENT_ELASTICSEARCH_HOST
+          value: "elasticsearch.logging.svc.cluster.local"
+        - name: FLUENT_ELASTICSEARCH_PORT
+          value: "9200"
+        volumeMounts:
+        - name: config
+          mountPath: /fluentd/etc
+        - name: varlog
+          mountPath: /var/log
+          readOnly: true
+      volumes:
+      - name: config
+        configMap:
+          name: fluentd-config
+      - name: varlog
+        hostPath:
+          path: /var/log
\ No newline at end of file
diff --git a/pytorch-training/production-training-template.yaml b/pytorch-training/production-training-template.yaml
new file mode 100644
index 0000000..2a9aada
--- /dev/null
+++ b/pytorch-training/production-training-template.yaml
@@ -0,0 +1,104 @@
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: pytorch-production-training
+  namespace: pytorch-training
+  labels:
+    app: pytorch-training
+    job-type: production
+    model: ${MODEL_NAME}
+    experiment: ${EXPERIMENT_ID}
+spec:
+  parallelism: ${PARALLEL_JOBS:-1}
+  completions: ${PARALLEL_JOBS:-1}
+  template:
+    metadata:
+      labels:
+        app: pytorch-training
+        job-type: production
+        model: ${MODEL_NAME}
+        experiment: ${EXPERIMENT_ID}
+    spec:
+      restartPolicy: Never
+      tolerations:
+      - key: nvidia.com/gpu
+        operator: Exists
+        effect: NoSchedule
+      nodeSelector:
+        accelerator: nvidia-tesla-t4  # Can be changed to more powerful GPUs for production
+      containers:
+      - name: pytorch-trainer
+        image: ${TRAINING_IMAGE}
+        command: ["python", "/app/train.py"]
+        args:
+        - --model=${MODEL_NAME}
+        - --experiment-id=${EXPERIMENT_ID}
+        - --epochs=${EPOCHS:-100}
+        - --batch-size=${BATCH_SIZE:-32}
+        - --learning-rate=${LEARNING_RATE:-0.001}
+        - --data-path=/datasets
+        - --output-path=/models
+        - --checkpoint-interval=${CHECKPOINT_INTERVAL:-10}
+        env:
+        - name: CUDA_VISIBLE_DEVICES
+          value: "0"
+        - name: NCCL_DEBUG
+          value: "INFO"
+        - name: PYTHONUNBUFFERED
+          value: "1"
+        resources:
+          requests:
+            memory: "${MEMORY_REQUEST:-8Gi}"
+            cpu: "${CPU_REQUEST:-4}"
+            nvidia.com/gpu: ${GPU_REQUEST:-1}
+          limits:
+            memory: "${MEMORY_LIMIT:-16Gi}"
+            cpu: "${CPU_LIMIT:-8}"
+            nvidia.com/gpu: ${GPU_LIMIT:-1}
+        volumeMounts:
+        - name: datasets
+          mountPath: /datasets
+          readOnly: true
+        - name: models
+          mountPath: /models
+        - name: checkpoints
+          mountPath: /checkpoints
+        - name: logs
+          mountPath: /logs
+      volumes:
+      - name: datasets
+        persistentVolumeClaim:
+          claimName: pytorch-datasets
+      - name: models
+        persistentVolumeClaim:
+          claimName: pytorch-models
+      - name: checkpoints
+        persistentVolumeClaim:
+          claimName: pytorch-checkpoints
+      - name: logs
+        emptyDir: {}
+  backoffLimit: 2
+---
+apiVersion: v1
+kind: PersistentVolumeClaim
+metadata:
+  name: pytorch-models
+  namespace: pytorch-training
+spec:
+  accessModes: ["ReadWriteOnce"]
+  resources:
+    requests:
+      storage: ${MODEL_STORAGE:-200Gi}
+  storageClassName: standard-rwo
+---
+apiVersion: v1
+kind: PersistentVolumeClaim
+metadata:
+  name: pytorch-checkpoints
+  namespace: pytorch-training
+spec:
+  accessModes: ["ReadWriteOnce"]
+  resources:
+    requests:
+      storage: ${CHECKPOINT_STORAGE:-500Gi}
+  storageClassName: standard-rwo
\ No newline at end of file
diff --git a/pytorch-training/sample-training-job.yaml b/pytorch-training/sample-training-job.yaml
new file mode 100644
index 0000000..6ebfad7
--- /dev/null
+++ b/pytorch-training/sample-training-job.yaml
@@ -0,0 +1,109 @@
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: pytorch-training-sample
+  namespace: pytorch-training
+  labels:
+    app: pytorch-training
+    job-type: sample
+spec:
+  template:
+    metadata:
+      labels:
+        app: pytorch-training
+        job-type: sample
+    spec:
+      restartPolicy: Never
+      tolerations:
+      - key: nvidia.com/gpu
+        operator: Exists
+        effect: NoSchedule
+      nodeSelector:
+        accelerator: nvidia-tesla-t4
+      containers:
+      - name: pytorch-trainer
+        image: pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel
+        command: ["python", "-c"]
+        args:
+        - |
+          import torch
+          import torch.nn as nn
+          import torch.optim as optim
+          import time
+          
+          print(f"PyTorch version: {torch.__version__}")
+          print(f"CUDA available: {torch.cuda.is_available()}")
+          if torch.cuda.is_available():
+              print(f"CUDA device count: {torch.cuda.device_count()}")
+              print(f"Current CUDA device: {torch.cuda.current_device()}")
+              print(f"CUDA device name: {torch.cuda.get_device_name()}")
+          
+          # Simple neural network for demonstration
+          class SimpleNet(nn.Module):
+              def __init__(self):
+                  super(SimpleNet, self).__init__()
+                  self.fc1 = nn.Linear(784, 128)
+                  self.fc2 = nn.Linear(128, 64)
+                  self.fc3 = nn.Linear(64, 10)
+                  self.relu = nn.ReLU()
+                  
+              def forward(self, x):
+                  x = self.relu(self.fc1(x))
+                  x = self.relu(self.fc2(x))
+                  x = self.fc3(x)
+                  return x
+          
+          # Create model and move to GPU if available
+          device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+          model = SimpleNet().to(device)
+          
+          # Create dummy data
+          batch_size = 64
+          input_size = 784
+          num_classes = 10
+          
+          # Simulate training
+          criterion = nn.CrossEntropyLoss()
+          optimizer = optim.Adam(model.parameters(), lr=0.001)
+          
+          print("Starting training simulation...")
+          for epoch in range(5):
+              # Generate random data
+              inputs = torch.randn(batch_size, input_size).to(device)
+              targets = torch.randint(0, num_classes, (batch_size,)).to(device)
+              
+              # Forward pass
+              outputs = model(inputs)
+              loss = criterion(outputs, targets)
+              
+              # Backward pass
+              optimizer.zero_grad()
+              loss.backward()
+              optimizer.step()
+              
+              print(f"Epoch {epoch+1}/5, Loss: {loss.item():.4f}")
+              time.sleep(2)  # Simulate training time
+          
+          print("Training completed successfully!")
+          print("GPU memory allocated:", torch.cuda.memory_allocated() if torch.cuda.is_available() else "N/A")
+        resources:
+          requests:
+            memory: "4Gi"
+            cpu: "2"
+            nvidia.com/gpu: 1
+          limits:
+            memory: "8Gi"
+            cpu: "4"
+            nvidia.com/gpu: 1
+        volumeMounts:
+        - name: datasets
+          mountPath: /datasets
+        - name: models
+          mountPath: /models
+      volumes:
+      - name: datasets
+        persistentVolumeClaim:
+          claimName: pytorch-datasets
+      - name: models
+        emptyDir: {}
+  backoffLimit: 3
\ No newline at end of file
diff --git a/scripts/scale-cluster.sh b/scripts/scale-cluster.sh
new file mode 100644
index 0000000..d60bb23
--- /dev/null
+++ b/scripts/scale-cluster.sh
@@ -0,0 +1,128 @@
+#!/bin/bash
+
+# Script to scale the PyTorch training cluster for production workloads
+
+set -e
+
+CLUSTER_NAME=""
+LOCATION=""
+NODE_POOL="gpu-pool"
+MIN_NODES=0
+MAX_NODES=10
+MACHINE_TYPE="n1-standard-8"
+GPU_TYPE="nvidia-tesla-v100"
+GPU_COUNT=1
+
+# Parse command line arguments
+while [[ $# -gt 0 ]]; do
+  case $1 in
+    --cluster-name)
+      CLUSTER_NAME="$2"
+      shift 2
+      ;;
+    --location)
+      LOCATION="$2"
+      shift 2
+      ;;
+    --min-nodes)
+      MIN_NODES="$2"
+      shift 2
+      ;;
+    --max-nodes)
+      MAX_NODES="$2"
+      shift 2
+      ;;
+    --machine-type)
+      MACHINE_TYPE="$2"
+      shift 2
+      ;;
+    --gpu-type)
+      GPU_TYPE="$2"
+      shift 2
+      ;;
+    --gpu-count)
+      GPU_COUNT="$2"
+      shift 2
+      ;;
+    --help)
+      echo "Usage: $0 --cluster-name CLUSTER_NAME --location LOCATION [OPTIONS]"
+      echo ""
+      echo "Options:"
+      echo "  --cluster-name    GKE cluster name (required)"
+      echo "  --location        GKE cluster location (required)"
+      echo "  --min-nodes       Minimum number of nodes (default: 0)"
+      echo "  --max-nodes       Maximum number of nodes (default: 10)"
+      echo "  --machine-type    Machine type for nodes (default: n1-standard-8)"
+      echo "  --gpu-type        GPU type (default: nvidia-tesla-v100)"
+      echo "  --gpu-count       Number of GPUs per node (default: 1)"
+      echo "  --help            Show this help message"
+      exit 0
+      ;;
+    *)
+      echo "Unknown option $1"
+      exit 1
+      ;;
+  esac
+done
+
+# Validate required arguments
+if [[ -z "$CLUSTER_NAME" || -z "$LOCATION" ]]; then
+  echo "Error: --cluster-name and --location are required"
+  echo "Use --help for usage information"
+  exit 1
+fi
+
+echo "Scaling cluster for production workloads..."
+echo "Cluster: $CLUSTER_NAME"
+echo "Location: $LOCATION"
+echo "Node pool: $NODE_POOL"
+echo "Min nodes: $MIN_NODES"
+echo "Max nodes: $MAX_NODES"
+echo "Machine type: $MACHINE_TYPE"
+echo "GPU type: $GPU_TYPE"
+echo "GPU count: $GPU_COUNT"
+
+# Update node pool configuration
+echo "Updating node pool autoscaling..."
+gcloud container clusters resize $CLUSTER_NAME \
+  --node-pool $NODE_POOL \
+  --num-nodes $MIN_NODES \
+  --location $LOCATION \
+  --quiet
+
+gcloud container node-pools update $NODE_POOL \
+  --cluster $CLUSTER_NAME \
+  --location $LOCATION \
+  --enable-autoscaling \
+  --min-nodes $MIN_NODES \
+  --max-nodes $MAX_NODES
+
+# Create a new production node pool if needed
+PROD_NODE_POOL="gpu-prod-pool"
+echo "Creating production node pool: $PROD_NODE_POOL"
+
+gcloud container node-pools create $PROD_NODE_POOL \
+  --cluster $CLUSTER_NAME \
+  --location $LOCATION \
+  --machine-type $MACHINE_TYPE \
+  --accelerator type=$GPU_TYPE,count=$GPU_COUNT \
+  --enable-autoscaling \
+  --min-nodes 0 \
+  --max-nodes $MAX_NODES \
+  --disk-size 200GB \
+  --disk-type pd-ssd \
+  --image-type COS_CONTAINERD \
+  --node-labels workload=production-training,accelerator=$GPU_TYPE \
+  --node-taints nvidia.com/gpu=present:NoSchedule \
+  --enable-autorepair \
+  --enable-autoupgrade \
+  --metadata disable-legacy-endpoints=true \
+  --scopes cloud-platform
+
+echo "Production scaling complete!"
+echo ""
+echo "Next steps:"
+echo "1. Update your training jobs to use the production node pool"
+echo "2. Set appropriate resource requests and limits"
+echo "3. Monitor cluster costs and usage"
+echo "4. Consider using preemptible instances for cost savings"
\ No newline at end of file
diff --git a/www/index.html b/www/index.html
index dacd505..d2fe33d 100644
--- a/www/index.html
+++ b/www/index.html
@@ -1,261 +1,117 @@
 <!DOCTYPE html>
 <html lang="en">
 <head>
-    <meta charset="utf-8">
-    <title>Dad Joke Generator - Powered by Pulumi</title>
-    <link rel="shortcut icon" href="favicon.png" type="image/png">
+    <meta charset="UTF-8">
     <meta name="viewport" content="width=device-width, initial-scale=1.0">
+    <title>PyTorch Training Cluster</title>
     <style>
-        * {
-            margin: 0;
-            padding: 0;
-            box-sizing: border-box;
-        }
-
         body {
-            font-family: 'Comic Sans MS', cursive, sans-serif;
-            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
-            min-height: 100vh;
-            display: flex;
-            align-items: center;
-            justify-content: center;
+            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
+            max-width: 800px;
+            margin: 0 auto;
             padding: 20px;
-        }
-
-        .container {
-            background: white;
-            border-radius: 20px;
-            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
-            padding: 40px;
-            text-align: center;
-            max-width: 600px;
-            width: 100%;
-            position: relative;
-            overflow: hidden;
-        }
-
-        .container::before {
-            content: '';
-            position: absolute;
-            top: -50%;
-            left: -50%;
-            width: 200%;
-            height: 200%;
-            background: repeating-linear-gradient(
-                45deg,
-                transparent,
-                transparent 10px,
-                rgba(255, 193, 7, 0.05) 10px,
-                rgba(255, 193, 7, 0.05) 20px
-            );
-            animation: float 20s ease-in-out infinite;
-            pointer-events: none;
-        }
-
-        @keyframes float {
-            0%, 100% { transform: translate(-50%, -50%) rotate(0deg); }
-            50% { transform: translate(-50%, -50%) rotate(180deg); }
-        }
-
-        .header {
-            position: relative;
-            z-index: 1;
-        }
-
-        h1 {
+            line-height: 1.6;
             color: #333;
-            font-size: 2.5rem;
-            margin-bottom: 10px;
-            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.1);
-        }
-
-        .emoji {
-            font-size: 3rem;
-            margin: 20px 0;
-            animation: bounce 2s infinite;
         }
-
-        @keyframes bounce {
-            0%, 20%, 50%, 80%, 100% { transform: translateY(0); }
-            40% { transform: translateY(-20px); }
-            60% { transform: translateY(-10px); }
+        .header {
+            text-align: center;
+            margin-bottom: 40px;
         }
-
-        .joke-container {
-            background: linear-gradient(135deg, #f8f9fa, #e9ecef);
-            border-radius: 15px;
-            padding: 30px;
-            margin: 30px 0;
-            border-left: 5px solid #ffc107;
-            min-height: 120px;
+        .logo {
+            width: 64px;
+            height: 64px;
+            margin: 0 auto 20px;
+            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
+            border-radius: 12px;
             display: flex;
             align-items: center;
             justify-content: center;
-            position: relative;
-            z-index: 1;
-        }
-
-        .joke-text {
-            font-size: 1.3rem;
-            line-height: 1.6;
-            color: #495057;
-            font-style: italic;
-            font-weight: 500;
-        }
-
-        .loading {
-            color: #6c757d;
-            font-style: normal;
-        }
-
-        .joke-button {
-            background: linear-gradient(135deg, #ffc107, #ff8c00);
             color: white;
-            border: none;
-            padding: 15px 30px;
-            font-size: 1.2rem;
-            border-radius: 50px;
-            cursor: pointer;
-            box-shadow: 0 4px 15px rgba(255, 193, 7, 0.4);
-            transition: all 0.3s ease;
+            font-size: 24px;
             font-weight: bold;
-            position: relative;
-            z-index: 1;
-        }
-
-        .joke-button:hover {
-            transform: translateY(-2px);
-            box-shadow: 0 6px 20px rgba(255, 193, 7, 0.6);
-        }
-
-        .joke-button:active {
-            transform: translateY(0);
         }
-
-        .joke-button:disabled {
-            opacity: 0.7;
-            cursor: not-allowed;
-            transform: none;
+        .status {
+            background: #f8f9fa;
+            border-radius: 8px;
+            padding: 20px;
+            margin: 20px 0;
         }
-
-        .footer {
-            margin-top: 30px;
-            padding-top: 20px;
-            border-top: 1px solid #dee2e6;
-            color: #6c757d;
-            font-size: 0.9rem;
-            position: relative;
-            z-index: 1;
+        .status-item {
+            display: flex;
+            justify-content: space-between;
+            margin: 10px 0;
         }
-
-        .footer a {
-            color: #8b5cf6;
-            text-decoration: none;
+        .status-value {
             font-weight: bold;
+            color: #28a745;
         }
-
-        .footer a:hover {
-            text-decoration: underline;
-        }
-
-        .timestamp {
-            font-size: 0.8rem;
-            color: #adb5bd;
-            margin-top: 10px;
+        .commands {
+            background: #f1f3f4;
+            border-radius: 8px;
+            padding: 20px;
+            font-family: 'Monaco', 'Menlo', monospace;
+            font-size: 14px;
+            overflow-x: auto;
         }
-
-        @media (max-width: 768px) {
-            .container {
-                margin: 10px;
-                padding: 20px;
-            }
-            
-            h1 {
-                font-size: 2rem;
-            }
-            
-            .joke-text {
-                font-size: 1.1rem;
-            }
+        .command {
+            margin: 10px 0;
+            color: #1a73e8;
         }
     </style>
 </head>
 <body>
-    <div class="container">
-        <div class="header">
-            <h1>🎭 Dad Joke Generator</h1>
-            <div class="emoji">😂</div>
+    <div class="header">
+        <div class="logo">🔥</div>
+        <h1>PyTorch Training Cluster</h1>
+        <p>Canonical development cluster ready for production-scale training</p>
+    </div>
+
+    <div class="status">
+        <h3>Cluster Status</h3>
+        <div class="status-item">
+            <span>Infrastructure:</span>
+            <span class="status-value">GKE with GPU Support</span>
+        </div>
+        <div class="status-item">
+            <span>GPU Type:</span>
+            <span class="status-value">NVIDIA T4 (Development)</span>
         </div>
-        
-        <div class="joke-container">
-            <div class="joke-text loading" id="joke-text">
-                Click the button below to get your first dad joke!
-            </div>
+        <div class="status-item">
+            <span>Framework:</span>
+            <span class="status-value">PyTorch 2.1.0</span>
         </div>
-        
-        <button class="joke-button" id="joke-button" onclick="getNewJoke()">
-            Get New Dad Joke! 🎲
-        </button>
-        
-        <div class="timestamp" id="timestamp"></div>
-        
-        <div class="footer">
-            <p>Powered by <a href="https://pulumi.com" target="_blank">Pulumi</a> ⚡️</p>
-            <p>Lambda jokes served fresh from AWS! ☁️</p>
+        <div class="status-item">
+            <span>Deployment:</span>
+            <span class="status-value">GitHub Actions CI/CD</span>
         </div>
     </div>
 
-    <script>
-        let isLoading = false;
-
-        async function getNewJoke() {
-            if (isLoading) return;
-            
-            isLoading = true;
-            const button = document.getElementById('joke-button');
-            const jokeText = document.getElementById('joke-text');
-            const timestamp = document.getElementById('timestamp');
-            
-            // Update button state
-            button.disabled = true;
-            button.textContent = '🎲 Loading...';
-            jokeText.textContent = 'Fetching a fresh dad joke...';
-            jokeText.className = 'joke-text loading';
-            timestamp.textContent = '';
-            
-            try {
-                const response = await fetch('source');
-                
-                if (!response.ok) {
-                    throw new Error(`HTTP ${response.status}: ${response.statusText}`);
-                }
-                
-                const data = await response.json();
-                
-                // Display the joke
-                jokeText.textContent = data.joke || 'Oops! The joke got lost in the cloud!';
-                jokeText.className = 'joke-text';
-                
-                // Show timestamp
-                if (data.timestamp) {
-                    const date = new Date(data.timestamp);
-                    timestamp.textContent = `Fresh from the cloud at ${date.toLocaleTimeString()}`;
-                }
-                
-            } catch (error) {
-                console.error('Error fetching joke:', error);
-                jokeText.textContent = `Oops! Something went wrong: ${error.message}. But here's a backup joke: Why don't scientists trust atoms? Because they make up everything! 🧪`;
-                jokeText.className = 'joke-text';
-            } finally {
-                // Reset button state
-                isLoading = false;
-                button.disabled = false;
-                button.textContent = 'Get New Dad Joke! 🎲';
-            }
-        }
+    <h3>Quick Start Commands</h3>
+    <div class="commands">
+        <div class="command"># Get cluster credentials</div>
+        <div>gcloud container clusters get-credentials pytorch-cluster --region us-central1</div>
+        <br>
+        <div class="command"># Run sample training job</div>
+        <div>kubectl apply -f pytorch-training/sample-training-job.yaml</div>
+        <br>
+        <div class="command"># Monitor training progress</div>
+        <div>kubectl logs job/pytorch-training-sample -n pytorch-training -f</div>
+        <br>
+        <div class="command"># Scale for production</div>
+        <div>./scripts/scale-cluster.sh --cluster-name pytorch-cluster --location us-central1</div>
+    </div>
 
-        // Load a joke when the page first loads
-        window.addEventListener('load', getNewJoke);
-    </script>
+    <h3>Architecture Highlights</h3>
+    <ul>
+        <li><strong>Cost-Optimized Development:</strong> Starts with 0 GPU nodes, scales on demand</li>
+        <li><strong>Production-Ready:</strong> Easy scaling to V100/A100 GPUs and larger clusters</li>
+        <li><strong>Secure by Default:</strong> Private cluster with workload identity</li>
+        <li><strong>Monitoring Built-in:</strong> Google Cloud Monitoring and custom metrics</li>
+        <li><strong>CI/CD Enabled:</strong> Automated deployments via GitHub Actions</li>
+    </ul>
+
+    <p style="text-align: center; margin-top: 40px; color: #666;">
+        Built with ❤️ using Pulumi Infrastructure as Code
+    </p>
 </body>
-</html>
+</html>
\ No newline at end of file
